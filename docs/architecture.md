# Architecture Overview

This document provides an overview of the system architecture, focusing on data storage and processing flow.

## Backend Structure

The backend is a Flask application responsible for:
- Receiving uploaded CSV files.
- Processing these files based on user-defined parameters.
- Generating an Excel report summarizing the comparison.
- Allowing users to download the generated report.

Key directories involved in data handling:

*   **`backend/uploads/`**:
    *   **Purpose**: This directory serves as a temporary (transient) storage location for CSV files uploaded by the user.
    *   **Lifecycle**:
        *   Files are saved here upon successful upload via the `/upload` endpoint.
        *   These files are read by the `/process` endpoint.
        *   After the `/process` request has finished handling these files (successfully or not), an attempt is made to delete them from this directory to free up space. This cleanup is done on a best-effort basis and failures in deletion are logged on the server but do not interrupt the user's request flow.
    *   **Implications**: This directory should not be relied upon for persistent storage. Its content is ephemeral.

*   **`backend/processed/`**:
    *   **Purpose**: This directory stores the generated Excel comparison reports.
    *   **Lifecycle**:
        *   Excel files are generated by the `/process` endpoint if data processing is successful.
        *   Each report is saved with a unique filename incorporating a timestamp (e.g., `comparison_report_YYYYMMDD_HHMMSS.xlsx`). This prevents filename collisions if multiple reports are generated.
        *   Currently, files in this directory persist on the server's filesystem.
    *   **Implications**:
        *   Users can download reports as long as they exist in this directory.
        *   For a production environment, a lifecycle management strategy for these reports is crucial. This could involve:
            *   Periodic cleanup scripts (e.g., a cron job) to delete reports older than a certain period (e.g., X days or hours).
            *   Manual cleanup by an administrator.
        *   The volume of reports could grow over time, consuming disk space if not managed.

## Data Flow Summary

1.  **Upload**: User selects CSV files via the frontend. Frontend `POST`s these files to `/upload`. Backend saves them to `backend/uploads/`.
2.  **Process**: User provides operation column and threshold. Frontend `POST`s uploaded filenames and parameters to `/process`.
    *   Backend reads CSVs from `backend/uploads/`.
    *   Data is validated and compared.
    *   An Excel report with a unique, timestamped name is generated and saved to `backend/processed/`.
    *   The unique report filename is returned to the frontend.
    *   Uploaded CSV files in `backend/uploads/` are deleted.
3.  **Download**: Frontend provides a download link using the unique report filename, pointing to `/download/<unique_filename>`. Backend serves the file from `backend/processed/`.

## Future Considerations for Scalability

The current local filesystem storage is suitable for small-scale or single-user instances. For larger datasets, multi-user environments, or more robust long-term storage requirements, the following enhancements should be considered:

*   **Database Integration**: For managing metadata about uploads, processing parameters, or even storing the data itself if it's structured and needs querying.
*   **Cloud Storage (e.g., AWS S3, Google Cloud Storage)**:
    *   For storing uploaded files and generated reports, offering better scalability, durability, and lifecycle management options (e.g., automatic deletion of old objects).
    *   This would decouple the application from the local filesystem and be essential for containerized or serverless deployments.
*   **Background Job Queues**: For long-running processing tasks, to prevent HTTP timeouts and improve responsiveness. The current implementation processes data synchronously within the HTTP request.

This architecture is designed to be simple for the current scope but highlights areas for expansion as requirements evolve.

## Testing

The backend includes a suite of unit tests for the CSV processing logic located in `backend/csv_handler.py`. These tests are built using the `pytest` framework.

### Running Backend Tests

To run the backend unit tests:

1.  **Navigate to the project root directory** (the directory containing the `backend` and `frontend` folders).
2.  **Install dependencies**: Ensure all backend dependencies, including `pytest` and `pytest-mock`, are installed. It's recommended to use a virtual environment.
    ```bash
    pip install -r backend/requirements.txt
    ```
3.  **Set PYTHONPATH**: To ensure Python can correctly resolve the project's modules, set or prepend the project root to the `PYTHONPATH`. From the project root:
    ```bash
    export PYTHONPATH=/path/to/your/project/root:$PYTHONPATH
    # Or, if you are in the project root:
    # export PYTHONPATH=$(pwd):$PYTHONPATH
    ```
    For the sandbox environment, `/app` is the project root:
    ```bash
    export PYTHONPATH=/app:$PYTHONPATH
    ```
4.  **Run pytest**: Execute `pytest` targeting the backend tests directory. You can add options like `-v` for verbose output.
    ```bash
    pytest backend/tests -v
    ```

The tests will then execute, providing a summary of passed, failed, and skipped tests. These tests cover various scenarios for CSV parsing, data validation, column comparison, and Excel report generation logic.
